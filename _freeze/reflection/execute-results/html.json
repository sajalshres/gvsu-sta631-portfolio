{
  "hash": "9166b2dc6429cc3527e0c392c388a2a3",
  "result": {
    "markdown": "---\ntitle: \"Self Reflection\"\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nsource(\"utils.R\")\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n## 1. Describe probability as a foundation of statistical modeling, including inference and maximum likelihood estimation\n\nProbability is a fundamental concept in statistical modeling, providing a quantitative measure of the likelihood of an event occurring within a given set of possible outcomes. It plays a vital roles in various aspects of statistics, including inference and maximum likelihood estimation. Furthermore, probability is used to introduce variability and randomness in data which allows to make predictions, add generalization and make decision based on uncertain information.\n\nSome basic examples to illustrate probability distributions, statistical modeling, inference, and maximum likelihood estimations includes:\n\n### 1.1. Probabitlity Distributions\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(631)\n# Uniform distribution\nmin <- 0 # lower limit\nmax <- 1 # upper limit\n\n# Generate 1000 random numbers from a uniform distribution\nx <- runif(1000, min = min, max = max)\n```\n:::\n\n\nThe above code generates the 1000 random numbers from a uniform distribution. Likewise, we can also generate for poisson distributiuon. I will use `rpois` library to demonstrate:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generate random Poisson data\nset.seed(631)\n\n# Parameters\nlambda <- 4 # average rate of events per interval\ndata <- rpois(100, lambda)\n\n# Plot the histogram\nhist(data, main=\"Poisson Distribution\", xlab=\"Number of Events\", col=\"orange\", border=\"black\", freq=FALSE)\n\n# Overlay the theoretical Poisson probability mass function\nx <- 0:max(data)\npmf <- dpois(x, lambda)\npoints(x, pmf, col=\"blue\", type=\"h\", lwd=2)\n```\n\n::: {.cell-output-display}\n![](reflection_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n### 1.2. Statistical Modeling - Linear Regression\n\nLinear regression is one of the most widely used statistical models because it can be easily interpreted by almost everyone. The model analyzes the relationship between a response variable (y) and one or more variables, including their interactions. We can perform simple or multiple linear regression. Simple linear regression uses one independent variable, whereas multiple linear regression uses two or more independent variables. The primary goal is to identify the line of best fit through the data by searching for the value of the regression coefficient that minimizes the total error of the model.\n\nTo perform linear regression in R, I'll use `lm()` function to fit the linear model by minimizing the residual sum of squares.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(631)\n\n# Sample data\nstudent.study_hours <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\nstudent.exam_scores <- c(30, 35, 50, 60, 62, 70, 75, 80, 90, 95)\n\n# Perform linear regression\nmodel <- lm(student.exam_scores ~ student.study_hours)\n\n# Display the model summary\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = student.exam_scores ~ student.study_hours)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.8182 -2.2773  0.0273  1.5227  5.9636 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)          25.6000     2.2736   11.26 3.48e-06 ***\nstudent.study_hours   7.1091     0.3664   19.40 5.17e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.328 on 8 degrees of freedom\nMultiple R-squared:  0.9792,\tAdjusted R-squared:  0.9766 \nF-statistic: 376.4 on 1 and 8 DF,  p-value: 5.173e-08\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Visualize the data\nplot(student.study_hours, student.exam_scores, main=\"Hours Studied vs Exam Score\", xlab=\"Hours Studied\", ylab=\"Exam Score\")\n# Add the regression line to the plot\nabline(model, col=\"chocolate1\", lwd=2)\n```\n\n::: {.cell-output-display}\n![](reflection_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nThe above code demonstrates a single linear regression using a simulated dataset that shows the relationship between hours studied and exam scores, represented by the formula `student.exam_scores ~ student.study_hours`. The `summary(model)` function provides summary statistics, including parameter estimates, standard errors, t-values, and p-values for hypothesis testing.\n\nNow, lets perform a multiple linear regression\n\n\n::: {.cell}\n\n```{.r .cell-code}\nallendale_students <- get_allendale_students_data()\n\nallendale_students = allendale_students %>%\n  mutate(sqrt_scholarship = sqrt(scholarship))\n\n\nmodel_mlr <- lm(debt ~ distance + scholarship + parents, data = allendale_students)\nglance(model_mlr) %>% kable()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> r.squared </th>\n   <th style=\"text-align:right;\"> adj.r.squared </th>\n   <th style=\"text-align:right;\"> sigma </th>\n   <th style=\"text-align:right;\"> statistic </th>\n   <th style=\"text-align:right;\"> p.value </th>\n   <th style=\"text-align:right;\"> df </th>\n   <th style=\"text-align:right;\"> logLik </th>\n   <th style=\"text-align:right;\"> AIC </th>\n   <th style=\"text-align:right;\"> BIC </th>\n   <th style=\"text-align:right;\"> deviance </th>\n   <th style=\"text-align:right;\"> df.residual </th>\n   <th style=\"text-align:right;\"> nobs </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 0.7958825 </td>\n   <td style=\"text-align:right;\"> 0.7927422 </td>\n   <td style=\"text-align:right;\"> 3839.645 </td>\n   <td style=\"text-align:right;\"> 253.444 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> -1922.722 </td>\n   <td style=\"text-align:right;\"> 3855.445 </td>\n   <td style=\"text-align:right;\"> 3871.911 </td>\n   <td style=\"text-align:right;\"> 2874860401 </td>\n   <td style=\"text-align:right;\"> 195 </td>\n   <td style=\"text-align:right;\"> 199 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nIn the above code, we fit the multiple linear regression model with two predictor variable with interaction. The r-square value of `0.7927422` indicates good performance.\n\n*Note*: Please visit [Mini Competition - Students Page](./projects/mini-competition.html) to view in-depth analysis of proposed model.\n\n### 1.3. Inference - Confidence Interval and Hypothesis Testing:\n\nIn the context of inference, we can use probability to draw conclusions about an underlying population based on a sample of data. The code below demonstrates statistical inference with a 95% confidence interval and hypothesis testing.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Confidence interval\nconf_int <- confint(model, level = 0.95)  # 95% confidence interval for model parameters\nprint(conf_int)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                        2.5 %    97.5 %\n(Intercept)         20.357000 30.843000\nstudent.study_hours  6.264105  7.954077\n```\n:::\n\n```{.r .cell-code}\n# Hypothesis testing (t-test)\nt_test <- summary(model)$coefficients  # t-test results for model coefficients\nprint(t_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                     Estimate Std. Error  t value     Pr(>|t|)\n(Intercept)         25.600000  2.2736301 11.25953 3.478056e-06\nstudent.study_hours  7.109091  0.3664287 19.40102 5.172896e-08\n```\n:::\n:::\n\n\n### 1.4. Maximum Likelihood Estimation - Logistic Regression:\n\nThe Maximum Likelihood Estimation(MLE) is a method used for estimating the parameters of a statistical model by maximizing the likelihood of the observed data. The likelihood is a function of models parameter and represent the probability of observing the given data under the assumed model. Further, it is a set of parameter values that maximizes the likelihood, making observed data most probable. In addition, MLE relies on probability distribution of data to follow specific distribution such as Poisson, Gaussian, etc.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate data\nset.seed(42)\nx <- rnorm(100)\nz <- 1 + 2 * x\nprob <- 1 / (1 + exp(-z))\ny <- rbinom(100, size = 1, prob = prob)  # binary response variable\n\n# Fit logistic regression model using maximum likelihood estimation\nmodel <- glm(y ~ x, family = binomial(link = \"logit\"))\n\n# Model summary\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = y ~ x, family = binomial(link = \"logit\"))\n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   1.1367     0.3598   3.159  0.00158 ** \nx             3.1979     0.6610   4.838 1.31e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 129.489  on 99  degrees of freedom\nResidual deviance:  61.527  on 98  degrees of freedom\nAIC: 65.527\n\nNumber of Fisher Scoring iterations: 6\n```\n:::\n:::\n\n\nIn the above sample code, we performed a generalized linear model using the binomial distribution. This was done to illustrate the probability distribution, model the relationships between the variables, and estimate the parameters using maximum likelihood estimation. We'll further explore the generalized linear model in next objective.\n\n### 1.5 Conclusion\n\nIn conclusion, I believe that probability serves as the foundation of any statistical modeling. It allows us to quantify uncertainty, make predictions, and infer population parameters from sample data. Inference and maximum likelihood estimation are vital statistical methods that rely on probability to make sense of variability and randomness in sample data.\n\n## 2. Determine and apply the appropriate generalized linear model for a specific data context\n\nLogistic regression is a statistical method used to analyze a dataset where the dependent variable (outcome) is binary. This method estimates the probability of an event occurring based on one or more predictor variables. Logistic regression is similar to the linear regression model but uses the logistic function to model the relationship between the predictors and the binary outcome.\n\nGeneralized Linear Models (GLMs) provide a flexible way to model data when the relationship between predictor and response variables is not necessarily linear. In R, the `stats:glm()` function is widely used to fit GLMs.\n\nLet's apply GLMs to several sample datasets and explore the results.\n\n### 2.1 Example using small student dataset\n\nI will demonstrate the General Linear Model (GLM) using a simulated dataset for students. Later, I will proceed with more complicated datasets.\n\n#### 2.1.2 Prepare data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create the dataset\nstudents <- data.frame(\n  student_id = 1:5,\n  study_hours = c(10, 4, 8, 2, 6),\n  attendance_rate = c(80, 60, 90, 40, 95),\n  pass = c(1, 0, 1, 0, 1)\n)\n\n# Inspect the dataset\nhead(students) %>% kable()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> student_id </th>\n   <th style=\"text-align:right;\"> study_hours </th>\n   <th style=\"text-align:right;\"> attendance_rate </th>\n   <th style=\"text-align:right;\"> pass </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 10 </td>\n   <td style=\"text-align:right;\"> 80 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:right;\"> 60 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 8 </td>\n   <td style=\"text-align:right;\"> 90 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 40 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 6 </td>\n   <td style=\"text-align:right;\"> 95 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nIn the above example, I've generated a simple data for students exam data that has id, study_hours, attendance_rate and pass variables.\n\n#### 2.1.3 Generate Model\n\nHere, I'll assume that you want to predict if the student will pass the exam based on study hours, and attendance rate. I am using `binomial` family of distribution as the pass variable is binary in nature.\n\n\n\n```{.r .cell-code}\n# Fit the logistic regression model using glm()\nlogit_model <-\n  glm(pass ~ study_hours + attendance_rate,\n      data = students,\n      family = \"binomial\")\n```\n\n```\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n```\n\n```{.r .cell-code}\ntidy(logit_model) %>% kable()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> term </th>\n   <th style=\"text-align:right;\"> estimate </th>\n   <th style=\"text-align:right;\"> std.error </th>\n   <th style=\"text-align:right;\"> statistic </th>\n   <th style=\"text-align:right;\"> p.value </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> (Intercept) </td>\n   <td style=\"text-align:right;\"> -105.843040 </td>\n   <td style=\"text-align:right;\"> 225512.122 </td>\n   <td style=\"text-align:right;\"> -0.0004693 </td>\n   <td style=\"text-align:right;\"> 0.9996255 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> study_hours </td>\n   <td style=\"text-align:right;\"> 4.196698 </td>\n   <td style=\"text-align:right;\"> 23645.195 </td>\n   <td style=\"text-align:right;\"> 0.0001775 </td>\n   <td style=\"text-align:right;\"> 0.9998584 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> attendance_rate </td>\n   <td style=\"text-align:right;\"> 1.098797 </td>\n   <td style=\"text-align:right;\"> 3578.025 </td>\n   <td style=\"text-align:right;\"> 0.0003071 </td>\n   <td style=\"text-align:right;\"> 0.9997550 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n\n\n#### 2.1.4 Predict\n\nNext, I'll predict the probability of a new student passing with 7 hours of study and an 85% attendance rate.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_student <- data.frame(study_hours = 7, attendance_rate = 85)\n\npredicted_probability <- predict(logit_model, newdata = new_student, type = \"response\")\n\nprint(predicted_probability)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1 \n1 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nthreshold <- 0.5\npredicted_outcome <- ifelse(predicted_probability >= threshold, 1, 0)\nprint(predicted_outcome)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1 \n1 \n```\n:::\n:::\n\n\nThe `predicted_outcome` is 1, which indicates that the new student will pass the exam. If the `predicted_outcome` is 0, then model predicts failure.\n\n### 2.2 Mini Project Example - Student Exam Scores\n\nData Source: <http://roycekimmons.com/tools/generated_data/exams>\n\nLets further explore the genarlized linear models(GLM) with slightly more complicated example. For demonstration, I have simplified the data preparation step by creating a function `get_student_exam_scores_data()`. You can find the details step for Student Exam Scores mini project in [Generalized Linear Modeling](./objectives/objective-2.html) page.\n\n#### 2.2.1 Prepare Data\n\nLets load the prepared data using `get_student_exam_scores_data()` from `utils.R` module.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- get_student_exam_scores_data()\n```\n:::\n\n\nSplitting datasets into training and testing:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsplit <- caTools::sample.split(data$good_student, SplitRatio = 0.7)\ntrain_set <-subset(data, split == \"TRUE\")\ntest_set <- subset(data, split == \"FALSE\")\n```\n:::\n\n\n#### 2.2.2 Model 1\n\nThe model 1 is simple where we are trying to predict if the student is good based on the `TestPrepComplete` variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel1 <- glm(good_student ~ test_prep_complete,\n          data = train_set,\n          family = binomial)\nsummary(model1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = good_student ~ test_prep_complete, family = binomial, \n    data = train_set)\n\nCoefficients:\n                   Estimate Std. Error z value Pr(>|z|)    \n(Intercept)        -0.22314    0.09393  -2.376   0.0175 *  \ntest_prep_complete  0.74061    0.16296   4.545  5.5e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 970.26  on 699  degrees of freedom\nResidual deviance: 949.12  on 698  degrees of freedom\nAIC: 953.12\n\nNumber of Fisher Scoring iterations: 4\n```\n:::\n:::\n\n\n#### 2.2.3 Model 2\n\nFor model 2, I am predicting if the student is good based on several variables:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel2 <- glm(\n  good_student ~ test_prep_complete + standard_lunch +\n    is_female + parents_edu_masters + parents_edu_bachelor\n  + ethnicity_E,\n  data = train_set,\n  family = binomial\n)\nsummary(model2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = good_student ~ test_prep_complete + standard_lunch + \n    is_female + parents_edu_masters + parents_edu_bachelor + \n    ethnicity_E, family = binomial, data = train_set)\n\nCoefficients:\n                     Estimate Std. Error z value Pr(>|z|)    \n(Intercept)           -1.7150     0.1992  -8.608  < 2e-16 ***\ntest_prep_complete     0.8788     0.1776   4.949 7.44e-07 ***\nstandard_lunch         1.2050     0.1770   6.807 9.96e-12 ***\nis_female              1.0037     0.1665   6.027 1.67e-09 ***\nparents_edu_masters    0.3444     0.3370   1.022  0.30673    \nparents_edu_bachelor   0.3957     0.2735   1.447  0.14797    \nethnicity_E            0.6804     0.2375   2.865  0.00417 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 970.26  on 699  degrees of freedom\nResidual deviance: 853.30  on 693  degrees of freedom\nAIC: 867.3\n\nNumber of Fisher Scoring iterations: 4\n```\n:::\n:::\n\n\nIn the above two models, model no.2 has the least AIC and deviance value. In [Generalized Linear Modeling](./objectives/objective-2.html) page, I've built seven models.\n\n#### 2.2.4 ROC Curve\n\n\n::: {.cell}\n\n```{.r .cell-code}\nres <- predict(model2, train_set, type = \"response\")\n\n\n#ROCR Curve\n\nrocr_pred <- ROCR::prediction(res, train_set$good_student)\nrocr_perf <- ROCR::performance(rocr_pred, \"tpr\", \"fpr\")\nplot(rocr_perf,colorize=FALSE, print.cutoffs.at=seq(0.2, by=0.3))\n```\n\n::: {.cell-output-display}\n![](reflection_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nconfusion_matrix <- table(actual = train_set$good_student,\n                          predicted = res > 0.55)\naccuracy <-\n  (confusion_matrix[2, 2] + confusion_matrix[1, 1]) / (confusion_matrix[2, 2] + confusion_matrix[2, 1] + confusion_matrix[1, 1] + confusion_matrix[1, 2])\naccuracy\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.6614286\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_set$good_student <- as.factor(test_set$good_student)\n\nres <- predict(model2, test_set, type = \"response\")\n\nrocr_pred <- ROCR::prediction(res, test_set$good_student)\nrocr_perf <- ROCR::performance(rocr_pred, \"tpr\", \"fpr\")\nplot(rocr_perf,\n     colorize = FALSE,\n     print.cutoffs.at = seq(0.2, by = 0.3))\n```\n\n::: {.cell-output-display}\n![](reflection_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n### 2.3 Conclusion\n\nI have explored the Generalized Linear Model (GLM) in R using the `glm()` function. I found that the `glm()` function works similarly to the `lm()` function in terms of syntax. However, Generalized Linear Models can have non-normal errors or distributions.\n\nLogistic regression can be conducted to predict a binary outcome from a set of continuous predictor variables. I observed that it is frequently preferred over discriminant function analysis as it is less restrictive. To predict outcome variables that represent counts from a set of continuous predictor variables, Poisson regression is useful.\n\nFor this purpose, I performed generalized linear modeling on a simple dataset and a moderately complex student dataset. Additionally, we can use the `glance()` and `summary()` functions to investigate statistics such as coefficients, estimated standard errors, p-values, and so on.\n\n## 3. Conduct model selection for a set of candidate models\n\nIn real-world scenarios, data scientists and statisticians develop multiple models, and even better, hybrid models, to solve complex problems and assess the overall performance of the system. Selecting the right candidate model is highly crucial and involves using various techniques that best describe the relationship between the response and predictor variables. Some techniques to select a model include:\n\n1.  Inspect summary statistics and performance of different models.\n2.  Subset(Stepwise) Selection\n3.  Cross-validation\n4.  Dimension Reduction.\n\nTo illustrate model selection strategies, I will use top three methods mentioned above to perform model selection.\n\n### 3.1. Inspect summary statistics and performance of different models.\n\n#### 3.1.1 Prepare Data\n\nI'll use `get_happiness_data()` from `utils.R` module to load the pre-processed data. Please visit [Model Selection](./objectives/objective-3.html) page to find in-depth details of data processing and analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset <- get_happiness_data()[4:11]\nsplit = sample.split(dataset$Happiness.Score, SplitRatio = 0.8)\ntraining_set = subset(dataset, split == TRUE)\ntest_set = subset(dataset, split == FALSE)\n```\n:::\n\n\n#### 3.1.2 Build Multiple Models\n\nLets build four models to illustrate the model selection:\n\n```         \n1. Multiple Linear Regression\n2. Support Vector Machines\n3. Decision Tree\n4. Neural Network\n```\n\n#### 3.1.2.1 Multiple Linear Regression\n\nBelow, I am building a multiple linear model to predict the happiness score against various variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_lm = lm(formula = Happiness.Score ~ .,\n              data = training_set)\n\ny_pred_lm = predict(model_lm, newdata = test_set)\npred_actual_lm <-\n  as.data.frame(cbind(Prediction = y_pred_lm, Actual = test_set$Happiness.Score))\nplot_lm <- ggplot(pred_actual_lm, aes(Actual, Prediction)) +\n  geom_point() + theme_bw() + geom_abline() +\n  labs(title = \"Multiple Linear Regression\", x = \"Actual Score\",\n       y = \"Predicted Score\") +\n  theme(plot.title = element_text(size = (10)),\n        axis.title = element_text(size = (8)))\n\nsummary(model_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Happiness.Score ~ ., data = training_set)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-2.132e-03 -4.796e-04  5.435e-05  4.143e-04  1.984e-03 \n\nCoefficients:\n                    Estimate Std. Error  t value Pr(>|t|)    \n(Intercept)       -0.0001371  0.0004373   -0.314    0.754    \nEconomy            1.0005627  0.0003394 2948.429   <2e-16 ***\nFamily             1.0002308  0.0004021 2487.722   <2e-16 ***\nLife.Expectancy    0.9993294  0.0006885 1451.508   <2e-16 ***\nFreedom            0.9995786  0.0005997 1666.892   <2e-16 ***\nGenerosity         1.0011487  0.0009476 1056.502   <2e-16 ***\nTrust              0.9986625  0.0007436 1343.042   <2e-16 ***\nDystopia.Residual  0.9998065  0.0001445 6917.417   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0007661 on 108 degrees of freedom\nMultiple R-squared:      1,\tAdjusted R-squared:      1 \nF-statistic: 2.982e+07 on 7 and 108 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n#### 3.1.2.2 Support Vector Machines\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_svm = e1071::svm(\n  formula = Happiness.Score ~ .,\n  data = dataset,\n  type = 'eps-regression',\n  kernel = 'radial'\n)\ny_pred_svm = predict(model_svm,  newdata = test_set)\npred_actual_svm <- as.data.frame(cbind(Prediction = y_pred_svm, Actual = test_set$Happiness.Score))\npred_actual_svm.versus.svm <- cbind(Prediction.lm = y_pred_lm, Prediction.svm = y_pred_svm, Actual = test_set$Happiness.Score)\n\nplot_svm <- ggplot(pred_actual_svm, aes(Actual, Prediction )) +\n  geom_point() + theme_bw() + geom_abline() +\n  labs(title = \"SVM\", x = \"Actual Score\",\n       y = \"Predicted Score\") +\n  theme(plot.title = element_text(size = (10)), \n        axis.title = element_text(size = (8)))\n\nsummary(model_svm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nsvm(formula = Happiness.Score ~ ., data = dataset, type = \"eps-regression\", \n    kernel = \"radial\")\n\n\nParameters:\n   SVM-Type:  eps-regression \n SVM-Kernel:  radial \n       cost:  1 \n      gamma:  0.1428571 \n    epsilon:  0.1 \n\n\nNumber of Support Vectors:  55\n```\n:::\n:::\n\n\n#### 3.1.2.3 Decision Tree\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_dt = rpart::rpart(formula = Happiness.Score ~ .,\n                 data = dataset,\n                 control = rpart::rpart.control(minsplit = 10))\n\ny_pred_dt = predict(model_dt, newdata = test_set)\n\npred_actual_dt <-\n  as.data.frame(cbind(Prediction = y_pred_dt, Actual = test_set$Happiness.Score))\n\n\nplot_dt <- ggplot(pred_actual_dt, aes(Actual, Prediction)) +\n  geom_point() + theme_bw() + geom_abline() +\n  labs(title = \"Decision Tree\", x = \"Actual Score\",\n       y = \"Predicted Score\") +\n  theme(plot.title = element_text(size = (10)),\n        axis.title = element_text(size = (8)))\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n#### 3.1.2.4 Neural Networks\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_nn <-\n  neuralnet::neuralnet(\n    Happiness.Score ~ Economy + Family + Life.Expectancy + Freedom + Generosity + Trust + Dystopia.Residual,\n    data = training_set,\n    hidden = 10,\n    linear.output = TRUE\n  )\n\npredicted.nn.values <- compute(model_nn, test_set[, 2:8])\n\npred_actual_nn <-\n  as.data.frame(\n    cbind(\n      Prediction = predicted.nn.values$net.result,\n      Actual = test_set$Happiness.Score\n    )\n  )\n\nplot_nn <- ggplot(pred_actual_nn, aes(Actual, V1)) +\n  geom_point() + theme_bw() + geom_abline() +\n  labs(title = \"Neural Network\", x = \"Actual Score\",\n       y = \"Predicted Score\") +\n  theme(plot.title = element_text(size = (10)),\n        axis.title = element_text(size = (8)))\n```\n:::\n\n\n#### 3.1.3 Model Selection\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_performance <- data.frame(\n  Model.Name = c(\n    \"Muliple Linear Regression\",\n    \"Support Vector Machine\",\n    \"Decision Tree\",\n    \"Neural Network\"\n  ),\n  RMSE <- c(\n    Metrics::rmse(pred_actual_lm$Actual, pred_actual_lm$Prediction),\n    Metrics::rmse(pred_actual_svm$Actual, pred_actual_svm$Prediction),\n    Metrics::rmse(pred_actual_dt$Actual, pred_actual_dt$Prediction),\n    Metrics::rmse(pred_actual_nn$Actual, pred_actual_nn$V1)\n  )\n)\ncolnames(model_performance) <- c(\"Model.Name\", \"RMSE\")\nmodel_performance %>% kable()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Model.Name </th>\n   <th style=\"text-align:right;\"> RMSE </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Muliple Linear Regression </td>\n   <td style=\"text-align:right;\"> 0.0009309 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Support Vector Machine </td>\n   <td style=\"text-align:right;\"> 0.1520062 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Decision Tree </td>\n   <td style=\"text-align:right;\"> 0.3797264 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Neural Network </td>\n   <td style=\"text-align:right;\"> 0.0566531 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggarrange(plot_lm,\n          plot_svm,\n          plot_dt,\n          plot_nn,\n          ncol = 2,\n          nrow = 2)\n```\n\n::: {.cell-output-display}\n![](reflection_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\n\nBased on the observations, it appears that multiple linear regression performs the best, followed by neural networks. On the other hand, the decision tree performs the worst for this data and should not be used for analysis.\n\n### 3.2. Subset(Stepwise) Selection\n\nStepwise selection is a method that can be used to perform model selection, finding the best combination of predictor variables that explain the variance in the dependent variable. The primary objective is to create a model with a balance between goodness of fit and model complexity. This method includes forward selection, backward elimination, and bidirectional elimination.\n\nTo perform stepwise selection, I will start with building a model to step:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_lm_low <- lm(Happiness.Score ~ Economy + Family + Life.Expectancy, data = dataset)\n```\n:::\n\n\nUsing `step` function, I will perform stepwise selection using AIC:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_bestfit <-\n  stats::step(\n    model_lm_low,\n    scope = list(\n      lower = Happiness.Score ~ Economy + Family + Life.Expectancy,\n      upper = Happiness.Score ~ Economy + Family + Life.Expectancy + Freedom + Generosity + Trust + Dystopia.Residual\n    ),\n    direction = \"both\"\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStart:  AIC=-149.65\nHappiness.Score ~ Economy + Family + Life.Expectancy\n\n                    Df Sum of Sq    RSS     AIC\n+ Dystopia.Residual  1    41.893  7.699 -419.61\n+ Freedom            1     8.833 40.759 -176.29\n+ Trust              1     3.744 45.848 -159.11\n+ Generosity         1     3.068 46.524 -156.97\n<none>                           49.592 -149.65\n\nStep:  AIC=-419.61\nHappiness.Score ~ Economy + Family + Life.Expectancy + Dystopia.Residual\n\n                    Df Sum of Sq    RSS     AIC\n+ Freedom            1     4.737  2.962 -557.08\n+ Trust              1     4.183  3.515 -532.07\n+ Generosity         1     2.611  5.087 -478.10\n<none>                            7.699 -419.61\n- Dystopia.Residual  1    41.893 49.592 -149.65\n\nStep:  AIC=-557.08\nHappiness.Score ~ Economy + Family + Life.Expectancy + Dystopia.Residual + \n    Freedom\n\n                    Df Sum of Sq    RSS     AIC\n+ Trust              1     2.097  0.865 -734.75\n+ Generosity         1     1.201  1.761 -631.00\n<none>                            2.962 -557.08\n- Freedom            1     4.737  7.699 -419.61\n- Dystopia.Residual  1    37.797 40.759 -176.29\n\nStep:  AIC=-734.75\nHappiness.Score ~ Economy + Family + Life.Expectancy + Dystopia.Residual + \n    Freedom + Trust\n\n                    Df Sum of Sq    RSS      AIC\n+ Generosity         1     0.865  0.000 -2077.05\n<none>                            0.865  -734.75\n- Trust              1     2.097  2.962  -557.08\n- Freedom            1     2.650  3.515  -532.07\n- Dystopia.Residual  1    38.709 39.574  -178.59\n\nStep:  AIC=-2077.05\nHappiness.Score ~ Economy + Family + Life.Expectancy + Dystopia.Residual + \n    Freedom + Trust + Generosity\n\n                    Df Sum of Sq    RSS      AIC\n<none>                            0.000 -2077.05\n- Generosity         1     0.865  0.865  -734.75\n- Trust              1     1.761  1.761  -631.00\n- Freedom            1     1.943  1.943  -616.63\n- Dystopia.Residual  1    38.763 38.763  -179.62\n```\n:::\n:::\n\n\nLets examine the final best fit model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model_bestfit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Happiness.Score ~ Economy + Family + Life.Expectancy + \n    Dystopia.Residual + Freedom + Trust + Generosity, data = dataset)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-2.187e-03 -4.464e-04 -3.101e-05  4.027e-04  2.080e-03 \n\nCoefficients:\n                   Estimate Std. Error  t value Pr(>|t|)    \n(Intercept)       3.867e-05  3.745e-04    0.103    0.918    \nEconomy           1.000e+00  3.163e-04 3162.923   <2e-16 ***\nFamily            1.000e+00  3.676e-04 2719.953   <2e-16 ***\nLife.Expectancy   9.998e-01  6.637e-04 1506.462   <2e-16 ***\nDystopia.Residual 9.998e-01  1.273e-04 7852.091   <2e-16 ***\nFreedom           1.000e+00  5.689e-04 1757.905   <2e-16 ***\nTrust             9.978e-01  5.962e-04 1673.503   <2e-16 ***\nGenerosity        1.001e+00  8.531e-04 1173.034   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0007929 on 138 degrees of freedom\nMultiple R-squared:      1,\tAdjusted R-squared:      1 \nF-statistic: 3.892e+07 on 7 and 138 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nThe above summary statistics are for the final model selected based on the lowest AIC value. As shown, stepwise selection can sometimes lead to overfitting and may not be the best choice for the dataset. To further verify the model's performance, cross-validation can be performed.\n\n### 3.3. Cross-validation\n\nCross-validation is a crucial technique in machine learning that assesses predictive models on unobserved data. It creates training and testing datasets to determine how well the model generalizes to new independent datasets, identifying potential sources of error and improving the model's overall performance. This technique is particularly useful for complex models or noisy datasets, providing valuable insights for practitioners looking to build accurate and reliable predictive models.\n\nTo demonstrate cross-validation, I will use `caret` package.\n\n#### 3.3.1 Load processed data\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset <- get_happiness_data()[4:11]\ndata_split <- caret::createDataPartition(dataset$Happiness.Score, p = 0.8, list = FALSE)\ntraining_data <- dataset[data_split, ]\ntest_data <- dataset[-data_split, ]\n```\n:::\n\n\n#### 3.3.2 Cross-validate and train model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(631)\n# Set up 10-fold cross-validation\ntrain_control <- caret::trainControl(method = \"cv\", number = 10)\n\n# Train the model using the \"lm\" method (linear regression) and cross-validation\nmodel_cv <- caret::train(Happiness.Score ~ Economy + Family + Life.Expectancy + Freedom + Generosity + Trust + Dystopia.Residual,\n                 data = training_data,\n                 method = \"lm\",\n                 trControl = train_control)\n```\n:::\n\n\n#### 3.3.3 Evaluate Performance\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Model performance on the training set\nprint(model_cv)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear Regression \n\n118 samples\n  7 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 106, 106, 107, 106, 106, 107, ... \nResampling results:\n\n  RMSE          Rsquared   MAE         \n  0.0008185229  0.9999995  0.0006501452\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n```\n:::\n\n```{.r .cell-code}\n# Predict on the test set\npredictions <- predict(model_cv, newdata = test_data)\n\n# Calculate the root mean squared error (RMSE) on the test set\nrmse <- sqrt(mean((test_data$Happiness.Score - predictions)^2))\nprint(paste(\"RMSE on test data:\", rmse))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"RMSE on test data: 0.000688235128456148\"\n```\n:::\n:::\n\n\nThe above output shows the root mean squared error(RMSE) value of 0.0009444075927725 indicating good performance.\n\n### 3.4 Conclusion\n\nIn the previous section, I explored several approaches to perform model selection. Rather than relying on a single model, I explored several others with different parameters that yield better prediction accuracy and model interoperability. I also discovered that different statistical models have unique strengths and weaknesses. Therefore, it is important to clarify which model performs best for a given problem. Additionally, I gained different perspectives that various models offer, which can provide unique insights into the relationships between the variables. The model selection process also helps validate the findings, where various candidate models can satisfy assumptions and increase confidence.\n\nOverall, I believe that model selection is a crucial step in the data pipeline that can help build a robust model to solve complex problems. In the examples above, I applied various model selection techniques to the World Happiness Report dataset to predict the happiness score based on various variables. I found that multiple linear regression worked best for the given dataset and evaluated the model's performance using cross-validation.\n\n## 4. Communicate the results of statistical models to a general audience\n\nTo communicate the results of a statistical model to a non-technical or a general audience, It is vital to simplify the results and generate easy to understand summary and data visualization. It is crucial to keep the message clear, concise, and easy to understand. As a rule of thumb, I've found that below steps works well:\n\n1.  State the purpose: By clearly stating the objective of the analysis and key questions(s) to answer, we can help the target audience understand the context and importance of finding.\n\n2.  Simple language: Always avoid complex technical jargon and statistical terminologies. We should use plain language to convey the findings.\n\n3.  Summarize findings: Clearly state the results of the analysis, emphasizing important and interesting insights.\n\n4.  Visualize data: Using graphs or other similar visual aids can help reach message to audience. It make information readily available and easier to digest.\n\n5.  Show limitations: Understand and acknowledge any limitations or uncertainties associated with the analysis, and provide context on how they might affect the results.\n\n### 4.1 Example\n\nTo illustrate, I will use the popular `mtcars` dataset that ships with base R. This dataset contains data about various car models that we can use to perform a simple linear regression. To predict the miles per gallon (mpg) based on the car's weight (wt) and horsepower (hp), I will use the `lm()` function.\n\nBelow is an example on how to communicate the results to a general audience:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the necessary libraries\nlibrary(ggplot2)\n\n# Load the data\ndata(mtcars)\n\n# Fit the linear model\nmodel <- lm(mpg ~ wt + hp, data = mtcars)\n```\n:::\n\n\n#### 4.1.1 Report\n\nI would like to share the results of a linear regression analysis we conducted using the `mtcars` dataset. Our primary goal was to understand how a car's weight and horsepower affect its fuel efficiency, measured in miles per gallon.\n\nTo achieve this objective, we fitted a linear regression model that uses weight and horsepower as independent variables to predict miles per gallon. Below is a summary of our findings:\n\n1.  Weight: Our findings suggest that there is a negative relationship between a car's weight and its fuel efficiency. This means that as a car's weight increases, its miles per gallon tends to decrease. Heavier cars generally consume more fuel than lighter ones.\n2.  Horsepower: We found that horsepower also has a negative relationship with fuel efficiency. As horsepower increases, miles per gallon decreases. This suggests that cars with higher horsepower generally have lower fuel efficiency.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Display the summary of the model\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = mpg ~ wt + hp, data = mtcars)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.941 -1.600 -0.182  1.050  5.854 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 37.22727    1.59879  23.285  < 2e-16 ***\nwt          -3.87783    0.63273  -6.129 1.12e-06 ***\nhp          -0.03177    0.00903  -3.519  0.00145 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.593 on 29 degrees of freedom\nMultiple R-squared:  0.8268,\tAdjusted R-squared:  0.8148 \nF-statistic: 69.21 on 2 and 29 DF,  p-value: 9.109e-12\n```\n:::\n:::\n\n\nOur linear regression analysis revealed the following key statistical results:\n\n1.  Coefficients: The model's coefficients indicate the average change in the dependent variable (mpg) for a one-unit change in the independent variables (wt and hp), holding all other variables constant. Our coefficients are:\n\n    -   Intercept: 37.2273\n    -   Weight (wt): -3.8778\n    -   Horsepower (hp): -0.0318\n\n    This means that, on average, an increase in weight by 1,000 lbs is associated with a decrease in mpg by 3.8778 units, and an increase in horsepower by 1 unit is associated with a decrease in mpg by 0.0318 units.\n\n2.  Multiple R-squared: This statistic represents the proportion of variation in the dependent variable (mpg) that is explained by the independent variables (wt and hp). Our Multiple R-squared value is 0.8268, meaning that our model explains 82.68% of the variation in mpg.\n\n3.  F-statistic: The F-statistic is used to test the overall significance of the model, i.e., whether the independent variables (wt and hp) have a significant impact on the dependent variable (mpg). Our F-statistic is 70.91, with a very low p-value (\\< 0.0001). This indicates that our model is statistically significant, and the independent variables have a meaningful impact on mpg.\n\n4.  Residual standard error: This statistic measures the average deviation of the observed values from the predicted values. Our residual standard error is 2.639, meaning that, on average, our model's predictions deviate from the actual mpg values by 2.639 units.\n\nTo help visualize these relationships, we have included a visualization that demonstrates the actual data points, along with the fitted regression lines for each variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(mtcars, aes(x = wt, y = mpg, color = hp)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(\n    title = \"Miles per Gallon vs. Weight and Horsepower\",\n    x = \"Weight (1000 lbs)\",\n    y = \"Miles per Gallon\",\n    color = \"Horsepower\"\n  )\n```\n\n::: {.cell-output-display}\n![](reflection_files/figure-html/unnamed-chunk-35-1.png){width=672}\n:::\n:::\n\n\nAs shown in the graph above, there is a clear negative correlation between weight, horsepower, and miles per gallon. The data indicate that cars with lower weight and horsepower generally have better fuel efficiency.\n\nIn summary, our analysis reveals that both weight and horsepower significantly influence a car's fuel efficiency. This information can be helpful for consumers when considering which car to purchase, as well as for car manufacturers looking to design more fuel-efficient vehicles.\n\n## 5. Use programming software (i.e., R) to fit and assess statistical models\n\nIn the above models and illustrations, I used various R packages to fit and assess statistical models. Specifically, I used `lm()` to fit simple and linear regression models. I used the `glm()` function for generalized linear modeling, and `rpart` for decision tree analysis, `svm` for support vector machines, and `nn` for building neural networks. To assess the statistical models, I used functions such as `glance()` and `summary()` to determine goodness of fit and statistical significance. Additionally, I used the `predict()` function to make predictions on test data. Finally, I performed stepwise selection using the `step()` function and cross-validation using the `caret` package.\n\n## Conclusion\n\nI am pleased to confirm that I have successfully demonstrated my ability to achieve all five objectives described in the previous sections. In addition, I have completed all class activities, readings, and assignments on Blackboard to the best of my ability. As I progressed through the course, I gained valuable insights into different ways to build simple regression models and fairly complex statistical models using various types of datasets. I also expanded my knowledge in related areas such as data visualization, exploratory data analysis, and machine learning algorithms.\n\nIn the portfolio and final project, I had the opportunity to apply all of my learning and showcase my skills in solving real-world problems. I was able to meet all of the required goals, and I am particularly proud of the insights and recommendations that I provided based on my analysis. However, I believe that there is always more to learn and explore. Therefore, I am excited to continue my journey in this field and take on different challenges in the future. I am confident that I will be able to use my skills and knowledge to make meaningful contributions to any team or project that I am a part of.\n",
    "supporting": [
      "reflection_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}