---
title: "Objective 2"
---

## 2. Generalized linear model

Course Objective:

> Determine and apply the appropriate generalized linear model for a specific data context

Logistic regression is a statistical method for analyzing a dataset in which the dependent variable(outcome) is a categorical with two levels. It helps to estimate the probability of an event occurring based on one or more predictor variables. In many ways, logistic regression is an extension of the linear regression model but uses the logistic function to model the relationship between the predictors and the binary outcome.

## 2.1 Example - Sudent Data

For this example, I will generate a small sample dataset to simulate the student data. I will use the data to fit a logistic regression model using R, and make predictions for a new student.

```{r import-lib, warning=FALSE, message=FALSE, echo=FALSE}
library("knitr")
library("kableExtra")
library("tidyverse")
library("tidymodels")
library("GGally")
library("psych")
library("ggfortify")
library("corrplot")
library("caTools")
library("ROCR")
```

### 2.1.1 Dataset

```{r}
# Create the dataset
students <- data.frame(
  student_id = 1:5,
  study_hours = c(10, 4, 8, 2, 6),
  attendance_rate = c(80, 60, 90, 40, 95),
  pass = c(1, 0, 1, 0, 1)
)

# Inspect the dataset
print(students)
```

### 2.1.2 Logistic Regression Model

```{r, results='asis'}
# Fit the logistic regression model using glm()
logit_model <- glm(pass ~ study_hours + attendance_rate, data = students, family = "binomial")


tidy(logit_model) %>% kable()
```

### 2.1.3 Predictions for New Student

```{r}
# Predict the probability of passing for a new student with 7 study hours and an 85% attendance rate
new_student <- data.frame(study_hours = 7, attendance_rate = 85)
predicted_probability <- predict(logit_model, newdata = new_student, type = "response")
print(predicted_probability)
```

```{r}
threshold <- 0.5
predicted_outcome <- ifelse(predicted_probability >= threshold, 1, 0)
print(predicted_outcome)
```

The `predicted_outcome` is 1, which indicates that the new student will pass the exam. If the `predicted_outcome` is 0, then model predicts failure.

## 2.2 Mini Project - Student Exam Scores

Data Source: <http://roycekimmons.com/tools/generated_data/exams>

### 2.2.1 Data Preparation

```{r}
# load the data and make strings as factors
exam_scores <-
  as_tibble(read.csv("../data/exam_scores.csv", stringsAsFactors = T))

exam_scores <-
  exam_scores %>% dplyr::rename(
    "ethnicity" = "race.ethnicity",
    "parental_education" = "parental.level.of.education",
    "test_preparation_course" = "test.preparation.course",
    "math_score" = "math.score",
    "reading_score" = "reading.score",
    "writing_score" = "writing.score"
  )
```

```{r}
head(exam_scores) %>% kable()
```

Inspect the variables:

```{r}
str(exam_scores)
```

Summary statistics"

```{r}
summary(exam_scores) %>% kable()
```

The data appears to be clean, as I did not observe any missing values, outliers, or NAs. Therefore, let's prepare the data to perform logistic regression models.

In below steps, I am adding additional diagnostics: .

```{r}
exam_scores <- exam_scores %>%
  rowwise() %>%
  mutate(average_score = mean(reading_score, writing_score, math_score))

exam_scores$good_student <-
  ifelse(exam_scores$average_score > mean(exam_scores$average_score),
         1,
         0)
```

Add further variables for gender, ethnicity, parental eduction, lunch types and test preb

```{r}
exam_scores$is_male <- ifelse(exam_scores$gender == "male", 1, 0)
exam_scores$is_female <-
  ifelse(exam_scores$gender == "female", 1, 0)

exam_scores$ethnicity_A <-
  ifelse(exam_scores$ethnicity == "group A", 1, 0)
exam_scores$ethnicity_B <-
  ifelse(exam_scores$ethnicity == "group B", 1, 0)
exam_scores$ethnicity_C <-
  ifelse(exam_scores$ethnicity == "group C", 1, 0)
exam_scores$ethnicity_D <-
  ifelse(exam_scores$ethnicity == "group D", 1, 0)
exam_scores$ethnicity_E <-
  ifelse(exam_scores$ethnicity == "group E", 1, 0)

exam_scores$parents_edu_associate <-
  ifelse(exam_scores$parental_education
         == "associate's degree",
         1,
         0)
exam_scores$parents_edu_bachelor <-
  ifelse(exam_scores$parental_education
         == "bachelor's degree",
         1,
         0)
exam_scores$parents_edu_hschool <-
  ifelse(
    exam_scores$parental_education
    == "high school" |
      exam_scores$parental_education ==
      "some high school",
    1,
    0
  )
exam_scores$parents_edu_masters <-
  ifelse(exam_scores$parental_education
         == "master's degree",
         1,
         0)
exam_scores$parents_edu_college <-
  ifelse(exam_scores$parental_education
         == "some college", 1, 0)

exam_scores$standard_lunch <-
  ifelse(exam_scores$lunch == "standard", 1, 0)
exam_scores$subsidised_lunch <-
  ifelse(exam_scores$lunch == "free/reduced", 1, 0)

exam_scores$test_prep_complete <-
  ifelse(exam_scores$test_preparation_course == "completed"
         , 1, 0)
head(exam_scores) %>% kable()
```

We can clean categorical variables as we have added diagnostic variable to build our model/

```{r}
data <- subset(
  exam_scores,
  select = -c(
    gender,
    ethnicity,
    parental_education,
    lunch,
    test_preparation_course
  )
)

data <- data[, c(6:20, 1:5)]
```

```{r}
#Corelation Analysis

cr <- cor(data)
corrplot(cr)
```

Now, lets do a forward regression and check significance of each variable as we continue adding to our model:

```{r}
split <- caTools::sample.split(data$good_student, SplitRatio = 0.8)
train_set <-subset(data, split == "TRUE")
test_set <- subset(data, split == "FALSE")
```

### 2.2.2 Logistic Regression Models

```{r}
model_low <- glm(good_student ~ test_prep_complete,
                 data = train_set,
                 family = binomial)
```

Using `step` function, I will perform stepwise selection using AIC:

```{r}
model_bestfit <-
  stats::step(
    model_low,
    scope = list(
      lower = good_student ~ test_prep_complete,
      upper = good_student ~ test_prep_complete + standard_lunch +
        is_female + parents_edu_masters + parents_edu_bachelor + ethnicity_E
    ),
    direction = "both"
  )

```

Lets examine the final best fit model:

```{r}
summary(model_bestfit)
```

The above summary statistics are for the final model selected based on the lowest AIC value.

### ROC Curve

```{r}
res <- predict(model_bestfit, train_set, type = "response")
rocr_pred <- ROCR::prediction(res, train_set$good_student)
rocr_perf <- ROCR::performance(rocr_pred, "tpr", "fpr")
```

```{r}
plot(rocr_perf,colorize=FALSE, print.cutoffs.at=seq(0.2, by=0.3))
```

```{r}
confusion_matrix <- table(actual = train_set$good_student,
                          predicted = res > 0.55)
accuracy <-
  (confusion_matrix[2, 2] + confusion_matrix[1, 1]) / (confusion_matrix[2, 2] + confusion_matrix[2, 1] + confusion_matrix[1, 1] + confusion_matrix[1, 2])
accuracy
```

```{r}
test_set$good_student <- as.factor(test_set$good_student)

res <- predict(model_bestfit, test_set, type = "response")

rocr_pred <- ROCR::prediction(res, test_set$good_student)
rocr_perf <- ROCR::performance(rocr_pred, "tpr", "fpr")
plot(rocr_perf,
     colorize = FALSE,
     print.cutoffs.at = seq(0.2, by = 0.3))
```
